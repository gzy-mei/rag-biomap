import sys
import os
import re
import argparse
import pandas as pd
import numpy as np
import jieba
from openai import OpenAI
from typing import List, Dict
#è¿›è¡Œæ•°æ®é›†å¤„ç†
from data_description.invoke_data_manipulaltion_basyxx import extract_name_columns_from_excel
from data_description.invoke_Non_standard_data import extract_first_row_to_csv
#è¿›è¡Œå‘é‡åŒ–
from Build_an_index.invoke_Build_index import get_embedding, build_index_from_csv
from Build_an_index.invoke_Non_standard_data_Build_index import vectorize_header_terms
#è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi



def debug_data():
    print("==== è°ƒè¯•å¼€å§‹ ====")

    # 1. æŸ¥çœ‹æ ‡å‡†æœ¯è¯­CSVæ–‡ä»¶è¡Œæ•°å’Œsheetåç§°åˆ†å¸ƒ
    df_standard = pd.read_csv("data_description/test/æ ‡å‡†æœ¯è¯­_ç—…æ¡ˆé¦–é¡µ.csv")
    print(f"æ ‡å‡†æœ¯è¯­CSVæ€»è¡Œæ•°: {len(df_standard)}")
    print("æ ‡å‡†æœ¯è¯­CSVä¸­å„sheetåç§°è®¡æ•°:")
    print(df_standard["sheetåç§°"].value_counts())

    # 2. æŸ¥çœ‹éæ ‡å‡†æ•°æ®CSVè¡Œæ•°
    df_header = pd.read_csv("data_description/test/header_row.csv", header=None)
    print(f"éæ ‡å‡†æ•°æ®CSVæ€»è¡Œæ•°: {len(df_header)}")

    # 3. æŸ¥çœ‹å‘é‡æ–‡ä»¶å†…å®¹æ•°é‡
    try:
        header_vectors = np.load("Build_an_index/test/header_terms.npy")
        print(f"header_vectorsæ•°é‡: {header_vectors.shape[0]}")
    except Exception as e:
        print(f"è¯»å–header_vectorsæ—¶å‡ºé”™: {e}")

    try:
        standard_vectors = np.load("Build_an_index/test/standard_terms.npy")
        print(f"standard_vectorsæ•°é‡: {standard_vectors.shape[0]}")
    except Exception as e:
        print(f"è¯»å–standard_vectorsæ—¶å‡ºé”™: {e}")

    # 4. è®¡ç®—ç›¸ä¼¼åº¦æ—¶æ‰“å°å¼‚å¸¸LLMé€‰æ‹©
    header_texts = df_header[0].tolist()
    standard_texts = df_standard["å†…å®¹"].dropna().astype(str).tolist()

    for h_text in header_texts:
        # è¿™é‡Œç®€å•æ‰“å°ï¼Œæ–¹ä¾¿è§‚å¯Ÿï¼ŒçœŸå®è°ƒè¯•ä¸­å¯æ”¾åˆ°ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°é‡Œ
        if not h_text or h_text.strip() == "":
            print(f"å¼‚å¸¸è¡¨å¤´æ–‡æœ¬ä¸ºç©º: '{h_text}'")

    print("==== è°ƒè¯•ç»“æŸ ====")

# è°ƒç”¨è¿™ä¸ªè°ƒè¯•å‡½æ•°
if __name__ == "__main__":
    debug_data()



# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://172.16.55.171:7010/v1",
    api_key="sk-cairi"
)

# é…ç½®å‚æ•°
CONFIG = {
    "llm_model": "CAIRI-LLM-reasoner",
    "non_standard_excel": "dataset/å¯¼å‡ºæ•°æ®ç¬¬1~1000æ¡æ•°æ®_ç—…æ¡ˆé¦–é¡µ-.xlsx",
    "standard_excel": "dataset/VTE-PTE-CTEPHç ”ç©¶æ•°æ®åº“.xlsx",
    "header_csv": "data_description/test/header_row.csv",
    "standard_terms_csv": "data_description/test/æ ‡å‡†æœ¯è¯­_ç—…æ¡ˆé¦–é¡µ.csv",
    "header_vectors": "Build_an_index/test/header_terms.npy",
    "standard_vectors": "Build_an_index/test/standard_terms.npy",
    "output_excel": "dataset/åŒ¹é…ç»“æœå¯¹æ¯”.xlsx",
    #"embedding_model": "nomic-embed-text",  # ä¿®æ”¹ä¸ºå½“å‰ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹åï¼šbge-m3ã€nomic-embed-textã€mxbai-embed-largeè¿˜éœ€è¦å†è°ƒç”¨å‡½æ•°ä¸­ä¿®æ”¹ï¼ï¼ï¼
    #"similarity_method": "BM25",  # ç›¸ä¼¼åº¦æ–¹æ³•ï¼Œæœ‰ï¼šBM25ï¼ŒCosine
    "output_dir": "dataset/Matching_Results_Comparison"

}


#åˆå§‹åŒ–ç›®å½•ç»“æ„ï¼š æ ¹æ®é…ç½®æ–‡ä»¶CONFIGä¸­çš„è·¯å¾„ï¼Œè‡ªåŠ¨åˆ›å»ºè¿™äº›è·¯å¾„æ‰€åœ¨çš„æ–‡ä»¶å¤¹ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨æ»´è¯ï¼‰
def initialize_directories():
    for path in [CONFIG["header_csv"], CONFIG["header_vectors"]]:
        os.makedirs(os.path.dirname(path), exist_ok=True)

#å¤„ç†éæ ‡å‡†æ•°æ®ï¼ˆè¡¨å¤´ä¿¡æ¯ï¼‰
def process_non_standard_data() -> List[str]:
    #from data_description.invoke_Non_standard_data import extract_first_row_to_csv
    # å…ˆè°ƒç”¨extract_first_row_to_csvï¼Œç¡®ä¿CSVç”ŸæˆæˆåŠŸ
    #ç»“æœç”Ÿæˆï¼šdata_description/test/header_row.csv
    if not extract_first_row_to_csv(CONFIG["non_standard_excel"], CONFIG["header_csv"]):
        raise RuntimeError("éæ ‡å‡†æ•°æ®å¤„ç†å¤±è´¥")

    # è°ƒç”¨å°è£…å¥½çš„å‘é‡åŒ–å‡½æ•°
    #from Build_an_index.invoke_Non_standard_data_Build_index import vectorize_header_terms
    #ç»“æœç”Ÿæˆï¼šBuild_an_index/test/header_terms.npy
    vectorize_header_terms(
        CONFIG["header_csv"],
        CONFIG["header_vectors"],
        failed_log_path="Build_an_index/test/header_terms_failed.csv"
    )

    # è¿”å›æ‰€æœ‰æ–‡æœ¬åˆ—è¡¨
    return pd.read_csv(CONFIG["header_csv"], header=None)[0].tolist()

#å¤„ç†æ ‡å‡†æœ¯è¯­æ•°æ®ï¼ˆçŸ¥è¯†åº“ï¼‰
def process_standard_data() -> List[str]:
    """
    å¤„ç†æ ‡å‡†æœ¯è¯­æ•°æ®ï¼šæå–æœ¯è¯­åˆ— â†’ ä¿å­˜CSV â†’ å‘é‡åŒ– â†’ è¿”å›æœ¯è¯­åˆ—è¡¨
    """
    # ç›´æ¥æå–æ ‡å‡†æœ¯è¯­sheetåç§°ã€åŸåˆ—åã€å†…å®¹
    # from data_description.invoke_data_manipulaltion_basyxx import extract_name_columns_from_excel
    #ç»“æœç”Ÿæˆï¼šdata_description/test/æ ‡å‡†æœ¯è¯­_ç—…æ¡ˆé¦–é¡µ.csv
    success = extract_name_columns_from_excel(
        CONFIG["standard_excel"],
        CONFIG["standard_terms_csv"],
        target_sheet="ç—…æ¡ˆé¦–é¡µä¿¡æ¯",
        target_column="åç§°"
    )
    if not success:
        raise RuntimeError("æ ‡å‡†æœ¯è¯­æå–å¤±è´¥")

    # åŠ è½½CSVå¹¶æå–æœ¯è¯­ï¼ˆå³â€œå†…å®¹â€åˆ—-ä¸ä¼šåŒ…å«â€œå†…å®¹â€è¿™ä¸¤ä¸ªå­—ã€‚ï¼‰
    df = pd.read_csv(CONFIG["standard_terms_csv"])
    if "å†…å®¹" not in df.columns:
        raise ValueError("æ ‡å‡†æœ¯è¯­CSVç¼ºå°‘ 'å†…å®¹' åˆ—")

    terms = df["å†…å®¹"].dropna().astype(str).tolist()
    print(f"âœ… æˆåŠŸåŠ è½½æ ‡å‡†æœ¯è¯­ï¼Œå…± {len(terms)} æ¡")

    # å‘é‡åŒ–â€œå†…å®¹â€åˆ—
    #ç»“æœç”Ÿæˆï¼šBuild_an_index/test/standard_terms.npy
    #from Build_an_index.invoke_Build_index import get_embedding, build_index_from_csv
    build_index_from_csv(
        CONFIG["standard_terms_csv"],     # ç›´æ¥ä½¿ç”¨åŸCSV
        CONFIG["standard_vectors"],       # ä¿å­˜å‘é‡è·¯å¾„
        column_index=2,                   # â€œå†…å®¹â€åˆ—åœ¨CSVä¸­çš„ä½ç½®
        verbose=False
    )

    return terms

# def generate_with_llm(prompt: str) -> str:
#     try:
#         response = client.chat.completions.create(
#             model=CONFIG["llm_model"],
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.1,
#             max_tokens=100
#         )
#         return response.choices[0].message.content.strip()
#     except Exception as e:
#         print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
#         return "[é»˜è®¤å›å¤]"


# def generate_with_llm(prompt: str) -> str:
#     try:
#         response = client.chat.completions.create(
#             model="CAIRI-LLM-reasoner",
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.1,
#             max_tokens=100
#         )
#
#         message_obj = response.choices[0].message
#         # ä¼˜å…ˆè¯»å– contentï¼Œå¦‚æœæ²¡æœ‰å°±è¯»å– reasoning_content
#         if message_obj.content:
#             return message_obj.content.strip()
#         elif hasattr(message_obj, "reasoning_content") and message_obj.reasoning_content:
#             return message_obj.reasoning_content.strip()
#         else:
#             return "[ç©ºå“åº”]"
#     except Exception as e:
#         print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
#         return "[é»˜è®¤å›å¤]"



# def generate_with_llm(prompt: str) -> str:
#     try:
#         response = client.chat.completions.create(
#             model=CONFIG["llm_model"],
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.1,
#             max_tokens=100
#         )
#
#         message_obj = response.choices[0].message
#         raw_content = message_obj.content.strip() if message_obj.content else ""
#         return raw_content
#     except Exception as e:
#         print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
#         return "[è°ƒç”¨å¤±è´¥]"

# def generate_with_llm(prompt: str) -> str:
#     try:
#         response = client.chat.completions.create(
#             model=CONFIG["llm_model"],
#             messages=[{"role": "user", "content": prompt}],
#             temperature=0.1,
#             max_tokens=100
#         )
#
#         # æ‰“å°å®Œæ•´å“åº”ç»“æ„ï¼
#         print("ğŸ“¦ LLMå®Œæ•´å“åº”ç»“æ„ï¼š")
#         print(response)
#
#         message_obj = response.choices[0].message
#         raw_content = message_obj.content.strip() if message_obj.content else ""
#         return raw_content
#     except Exception as e:
#         print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
#         return "[è°ƒç”¨å¤±è´¥]"


def generate_with_llm(prompt: str) -> str:
    try:
        response = client.chat.completions.create(
            model=CONFIG["llm_model"],
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=100
        )

        message_obj = response.choices[0].message

        # æå– LLM è¿”å›å†…å®¹ï¼ˆå…¼å®¹ CAIRI çš„ reasoning_content å­—æ®µï¼‰
        raw_content = None
        if hasattr(message_obj, "content") and message_obj.content:
            raw_content = message_obj.content.strip()
        elif hasattr(message_obj, "reasoning_content") and message_obj.reasoning_content:
            raw_content = message_obj.reasoning_content.strip()
        else:
            raw_content = ""

        print(f"ğŸ“¨ è¡¨å¤´ï¼š{prompt.split('åŸå§‹è¡¨å¤´ï¼š')[-1].splitlines()[0]}")
        print(f"ğŸ¯ LLMåŸå§‹è¿”å›ï¼š'{raw_content}'")
        print(f"ğŸ“¦ LLMå®Œæ•´å“åº”ç»“æ„ï¼š\n{response}")
        print("ğŸ“ ========= LLM è°ƒç”¨è®°å½• =========")

        # æå–ç¼–å· 1~4
        match = re.search(r'\b([1-4])\b', raw_content)
        if match:
            llm_choice = match.group(1)
        else:
            llm_choice = "N/A"

        return llm_choice

    except Exception as e:
        print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
        return "N/A"


def detect_similarity_method(func):
    def wrapper(*args, **kwargs):
        method_name = func.__name__.lower()
        if "bm25" in method_name:
            CONFIG["similarity_method"] = "BM25"
        elif "cosine" in method_name:
            CONFIG["similarity_method"] = "Cosine"
        else:
            CONFIG["similarity_method"] = "Unknown"
        return func(*args, **kwargs)
    return wrapper

# =========================
# âš™ï¸ é˜ˆå€¼é…ç½®ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
# =========================
threshold_ratio = 0.35  # è¡¨ç¤ºç›¸ä¼¼åº¦æœ€é«˜å¾—åˆ†çš„35%


#bm25
@detect_similarity_method
def calculate_similarities_bm25() -> List[Dict]:
    header_texts = pd.read_csv(CONFIG["header_csv"], header=None)[0].dropna().astype(str).tolist()
    standard_texts = pd.read_csv(CONFIG["standard_terms_csv"])["å†…å®¹"].dropna().astype(str).tolist()

    tokenized_corpus = [list(jieba.cut(text)) for text in standard_texts]
    bm25 = BM25Okapi(tokenized_corpus)

    results = []


    for h_text in header_texts:
        query = list(jieba.cut(h_text))
        scores = bm25.get_scores(query)
        top_3_indices = np.argsort(scores)[-3:][::-1]
        top_3 = [standard_texts[i] for i in top_3_indices]
        top_scores = [scores[i] for i in top_3_indices]

        # # åˆ¤æ–­æ˜¯å¦ä½äºé˜ˆå€¼
        # if top_scores[0] < max(scores) * threshold_ratio:
        #     llm_choice_result = ""  # ä¸è°ƒç”¨LLMï¼Œç›´æ¥ç©ºå­—ç¬¦ä¸²
        # else:
        #     prompt = f"""è¯·æ ¹æ®ç—…å†è¡¨å¤´é€‰æ‹©æœ€åŒ¹é…çš„æ ‡å‡†æœ¯è¯­ï¼ˆå¦‚æœæ²¡æœ‰åˆé€‚çš„è¯·é€‰4ï¼‰ï¼š
        #     åŸå§‹è¡¨å¤´ï¼š{h_text}
        #     å€™é€‰æœ¯è¯­ï¼š
        #     {chr(10).join(f'{i + 1}. {text}' for i, text in enumerate(top_3))}
        #     4. æ— ä¸€ä¸ªå€™é€‰é¡¹åŒ¹é…
        #
        #     è¯·åªè¿”å›é€‰æ‹©çš„ç¼–å·(1-4)ï¼Œä¸è¦è§£é‡Šã€‚"""
        #
        #     llm_choice = generate_with_llm(prompt)
        #     if llm_choice.isdigit() and 1 <= int(llm_choice) <= 3:
        #         llm_choice_result = top_3[int(llm_choice) - 1]
        #     elif llm_choice.strip() == "4":
        #         llm_choice_result = ""
        #     else:
        #         llm_choice_result = "N/A"

        # åˆ¤æ–­æ˜¯å¦ä½äºé˜ˆå€¼
        if top_scores[0] < max(scores) * threshold_ratio:
            llm_choice_result = ""  # ä¸è°ƒç”¨LLMï¼Œç›´æ¥ç©ºå­—ç¬¦ä¸²
        else:
            prompt = f"""è¯·æ ¹æ®ç—…å†è¡¨å¤´é€‰æ‹©æœ€åŒ¹é…çš„æ ‡å‡†æœ¯è¯­ï¼ˆå¦‚æœæ²¡æœ‰åˆé€‚çš„è¯·é€‰4ï¼‰ï¼š
        åŸå§‹è¡¨å¤´ï¼š{h_text}
        å€™é€‰æœ¯è¯­ï¼š
        {chr(10).join(f'{i + 1}. {text}' for i, text in enumerate(top_3))}
        4. æ— ä¸€ä¸ªå€™é€‰é¡¹åŒ¹é…

        è¯·åªè¿”å›é€‰æ‹©çš„ç¼–å·(1-4)ï¼Œä¸è¦è§£é‡Šã€‚"""

            llm_choice = generate_with_llm(prompt)

            # ğŸ è°ƒè¯•è¾“å‡º
            print("ğŸ“ ========= LLM è°ƒç”¨è®°å½• =========")
            print(f"ğŸ“¨ è¡¨å¤´ï¼š{h_text}")
            print(f"ğŸ¯ LLMåŸå§‹è¿”å›ï¼š'{llm_choice}'")

            # æ­£åˆ™æå– 1-4 çš„ç¼–å·
            match = re.search(r"\b([1-4])\b", llm_choice)
            if match:
                choice_num = int(match.group(1))
                if choice_num == 4:
                    llm_choice_result = ""
                else:
                    llm_choice_result = top_3[choice_num - 1]
            else:
                llm_choice_result = "N/A"

        results.append({
            "åŸå§‹è¡¨å¤´": h_text,
            "å€™é€‰æœ¯è¯­": top_3,
            "LLMé€‰æ‹©": llm_choice_result,
            "æœ€é«˜ç›¸ä¼¼åº¦": top_scores[0],
            "å¹³å‡ç›¸ä¼¼åº¦": np.mean(top_scores)
        })

    return results


#ç»“æœä¿å­˜å‡½æ•°
def save_results(results: List[Dict]):
    df = pd.DataFrame(results)

    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    df.drop(columns=[col for col in ["å¹³å‡ç›¸ä¼¼åº¦", "åŒ¹é…æˆåŠŸ"] if col in df.columns], inplace=True)

    # åŠ è½½ GT æ ‡å‡†ç­”æ¡ˆï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
    gt_path = "/home/gzy/rag-biomap/dataset/GT.xlsx"
    gt_df = pd.read_excel(gt_path, header=0)

    if gt_df.shape[1] < 2:
        raise ValueError("GT.xlsx å¿…é¡»è‡³å°‘åŒ…å«ä¸¤åˆ—ï¼Œç¬¬äºŒåˆ—ä¸ºæ ‡å‡†ç­”æ¡ˆ")

    gt_answers = gt_df.iloc[:, 1].fillna("").astype(str).tolist()
    df["GTæ ‡å‡†ç­”æ¡ˆ"] = pd.Series(gt_answers[:len(df)])

    # åŒ¹é…åˆ¤æ–­
    df["æ˜¯å¦åŒ¹é…GT"] = df.apply(lambda row: row["LLMé€‰æ‹©"] == row["GTæ ‡å‡†ç­”æ¡ˆ"], axis=1)

    # ç»Ÿè®¡ä¿¡æ¯
    total_accuracy = df["æ˜¯å¦åŒ¹é…GT"].mean()
    gt_empty_count = sum(df["GTæ ‡å‡†ç­”æ¡ˆ"] == "")
    llm_empty = df["LLMé€‰æ‹©"] == ""
    gt_empty = df["GTæ ‡å‡†ç­”æ¡ˆ"] == ""
    llm_not_empty = df["LLMé€‰æ‹©"] != ""

    llm_empty_and_gt_empty = df[llm_empty & gt_empty].shape[0]
    llm_empty_total = llm_empty.sum()
    llm_not_empty_total = llm_not_empty.sum()
    llm_not_empty_gt_empty = df[llm_not_empty & gt_empty].shape[0]
    llm_empty_gt_not_empty = df[llm_empty & ~gt_empty].shape[0]

    # åˆ é™¤â€œæ˜¯å¦åŒ¹é…GTâ€åˆ—ï¼Œç»“æœä¸­ä¸ä¿ç•™
    df.drop(columns=["æ˜¯å¦åŒ¹é…GT"], inplace=True)

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯åˆ° DataFrame çš„å°¾éƒ¨
    stats = pd.DataFrame([
        ["llmé€‰æ‹©ä¸GTæ ‡å‡†ç­”æ¡ˆåŒ¹é…å‡†ç¡®ç‡", total_accuracy],
        ["GTæ ‡å‡†ç­”æ¡ˆä¸­ç©ºå€¼ä¸ªæ•°", gt_empty_count],
        ["llmé€‰æ‹©ä¸ºç©ºï¼ŒGTä¹Ÿä¸ºç©ºçš„åŒ¹é…æˆåŠŸæ•°é‡", llm_empty_and_gt_empty],
        ["llmé€‰æ‹©ä¸ºç©ºçš„æ•°é‡", llm_empty_total],
        ["llmé€‰æ‹©éç©ºçš„æ•°é‡", llm_not_empty_total],
        ["llmé€‰æ‹©éç©ºï¼Œä½†GTæ˜¯ç©ºçš„æ•°é‡", llm_not_empty_gt_empty],
        ["llmé€‰æ‹©ä¸ºç©ºï¼ŒGTä¸ä¸ºç©ºçš„æ•°é‡", llm_empty_gt_not_empty]
    ], columns=df.columns[:2])  # ç”¨å‰ä¸¤åˆ—å¯¹é½è¡¨å¤´

    df_final = pd.concat([df, stats], ignore_index=True)

    # ä¿å­˜è·¯å¾„
    output_dir = "/home/gzy/rag-biomap/dataset/Matching_Results_Comparison"
    os.makedirs(output_dir, exist_ok=True)
    filename = "é˜ˆå€¼è®¾ç½®35.xlsx"
    output_path = os.path.join(output_dir, filename)

    # ä¿å­˜ Excel
    df_final.to_excel(output_path, index=False, engine="openpyxl")

    # æ§åˆ¶å°è¾“å‡ºï¼ˆè¾…åŠ©ç¡®è®¤ï¼‰
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {output_path}ï¼Œå…± {len(df)} æ¡è®°å½• + ç»Ÿè®¡ä¿¡æ¯")
    print(f"ğŸ“Š åŒ¹é…å‡†ç¡®ç‡ï¼š{total_accuracy:.6f}")
    print(f"ğŸ“Š GTä¸ºç©ºå€¼ï¼š{gt_empty_count}ï¼Œllmé€‰æ‹©ä¸ºç©ºæ•°é‡ï¼š{llm_empty_total}")
    print(f"ğŸ“Š llmé€‰æ‹©ä¸ºç©º && GTä¸ºç©ºï¼ˆåŒ¹é…ï¼‰ï¼š{llm_empty_and_gt_empty}")
    print(f"ğŸ“Š llmé€‰æ‹©éç©º && GTä¸ºç©ºï¼š{llm_not_empty_gt_empty}")
    print(f"ğŸ“Š llmé€‰æ‹©ä¸ºç©º && GTéç©ºï¼š{llm_empty_gt_not_empty}")





def main():
    from Build_an_index.invoke_Build_index import get_embedding


    #è‡ªåŠ¨è¯†åˆ«å½“å‰ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹--æ–¹æ³•ï¼šè·å–get_embeddingå‡½æ•°çš„æ¥æºæ¨¡å—è·¯å¾„ï¼ˆæ¯”å¦‚ bge_m3.pyï¼‰
    module_path = get_embedding.__module__

    print(f"ğŸ“ æ£€æµ‹åˆ° get_embedding æ¥è‡ªæ¨¡å—ï¼š{module_path}")  # âœ… ï¼šè°ƒè¯•è¾“å‡º

    if "bge_m3" in module_path:
        CONFIG["embedding_model"] = "bge-m3"
    elif "nomic_embed_text" in module_path:
        CONFIG["embedding_model"] = "nomic-embed-text"
    elif "mxbai_embed_large" in module_path:
        CONFIG["embedding_model"] = "mxbai-embed-large"
    else:
        CONFIG["embedding_model"] = "unknown"

    #CONFIG["embedding_model"] = get_embedding.__defaults__[0]

    # è‡ªåŠ¨æ£€æµ‹å½“å‰å¯ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°æ˜¯å“ªä¸€ä¸ª-é€šè¿‡å…¨å±€å˜é‡è¡¨globals()æ¥æ£€æµ‹å“ªä¸ªå‡½æ•°å­˜åœ¨ï¼Œæ˜¯æˆ‘æœ‰æ²¡æœ‰â€œå®šä¹‰â€å®ƒã€‚
    if "calculate_similarities_bm25" in globals():
        calculate_similarities = calculate_similarities_bm25
    elif "calculate_similarities_cosine" in globals():
        calculate_similarities = calculate_similarities_cosine
    else:
        raise ValueError(
            "æœªæ£€æµ‹åˆ°ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼Œè¯·ç¡®ä¿ä¿ç•™ calculate_similarities_bm25 æˆ– calculate_similarities_cosine ä¸­çš„ä¸€ä¸ª")

    initialize_directories()
    print("ğŸ”„ å¤„ç†éæ ‡å‡†æ•°æ®...")
    header_texts = process_non_standard_data()

    print("ğŸ”„ å¤„ç†æ ‡å‡†çŸ¥è¯†åº“...")
    standard_texts = process_standard_data()

    print("ğŸ” è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ‰§è¡Œç®€åŒ–RAGæµç¨‹...")
    results = calculate_similarities()

#è¿è¡Œç»“æœæ˜¾ç¤ºï¼šğŸ“ å½“å‰ä½¿ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ä¸ºï¼š......
    print(f"ğŸ“ å½“å‰ä½¿ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ä¸ºï¼š{CONFIG['similarity_method']}")

    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_results(results)

if __name__ == "__main__":
    main()
