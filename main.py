import sys
import os

# å°†æ–°çš„æ¨¡å—è·¯å¾„æ·»åŠ åˆ°sys.pathï¼Œæ–¹ä¾¿import
sys.path.extend([
    "/home/gzy/rag-biomap/data_description",        # ä½ çš„data_descriptionç›®å½•
    "/home/gzy/rag-biomap/Build_an_index",          # Build_an_indexç›®å½•
])

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from data_description.invoke_Non_standard_data import extract_first_row_to_csv
from Build_an_index.invoke_Build_index import get_embedding, build_index_from_csv
from data_description.invoke_data_manipulaltion_basyxx import extract_name_columns_from_excel
from openai import OpenAI
from typing import List, Dict
from Build_an_index.invoke_Non_standard_data_Build_index import vectorize_header_terms
from rank_bm25 import BM25Okapi
import jieba


def debug_data():
    print("==== è°ƒè¯•å¼€å§‹ ====")

    # 1. æŸ¥çœ‹æ ‡å‡†æœ¯è¯­CSVæ–‡ä»¶è¡Œæ•°å’Œsheetåç§°åˆ†å¸ƒ
    df_standard = pd.read_csv("/home/gzy/rag-biomap/data_description/test/æ ‡å‡†æœ¯è¯­_ç—…æ¡ˆé¦–é¡µ.csv")
    print(f"æ ‡å‡†æœ¯è¯­CSVæ€»è¡Œæ•°: {len(df_standard)}")
    print("æ ‡å‡†æœ¯è¯­CSVä¸­å„sheetåç§°è®¡æ•°:")
    print(df_standard["sheetåç§°"].value_counts())

    # 2. æŸ¥çœ‹éæ ‡å‡†æ•°æ®CSVè¡Œæ•°
    df_header = pd.read_csv("/home/gzy/rag-biomap/data_description/test/header_row.csv", header=None)
    print(f"éæ ‡å‡†æ•°æ®CSVæ€»è¡Œæ•°: {len(df_header)}")

    # 3. æŸ¥çœ‹å‘é‡æ–‡ä»¶å†…å®¹æ•°é‡
    try:
        header_vectors = np.load("/home/gzy/rag-biomap/Build_an_index/test/header_terms.npy")
        print(f"header_vectorsæ•°é‡: {header_vectors.shape[0]}")
    except Exception as e:
        print(f"è¯»å–header_vectorsæ—¶å‡ºé”™: {e}")

    try:
        standard_vectors = np.load("/home/gzy/rag-biomap/Build_an_index/test/standard_terms.npy")
        print(f"standard_vectorsæ•°é‡: {standard_vectors.shape[0]}")
    except Exception as e:
        print(f"è¯»å–standard_vectorsæ—¶å‡ºé”™: {e}")

    # 4. è®¡ç®—ç›¸ä¼¼åº¦æ—¶æ‰“å°å¼‚å¸¸LLMé€‰æ‹©
    header_texts = df_header[0].tolist()
    standard_texts = df_standard["å†…å®¹"].dropna().astype(str).tolist()

    for h_text in header_texts:
        # è¿™é‡Œç®€å•æ‰“å°ï¼Œæ–¹ä¾¿è§‚å¯Ÿï¼ŒçœŸå®è°ƒè¯•ä¸­å¯æ”¾åˆ°ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°é‡Œ
        if not h_text or h_text.strip() == "":
            print(f"å¼‚å¸¸è¡¨å¤´æ–‡æœ¬ä¸ºç©º: '{h_text}'")

    print("==== è°ƒè¯•ç»“æŸ ====")

# è°ƒç”¨è¿™ä¸ªè°ƒè¯•å‡½æ•°ï¼Œæ”¾åœ¨mainå‡½æ•°çš„å¼€å§‹æˆ–ä½ æƒ³æ£€æŸ¥çš„ä½ç½®
if __name__ == "__main__":
    debug_data()



# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://172.16.55.171:7010/v1",
    api_key="sk-cairi"
)

# é…ç½®å‚æ•°
CONFIG = {
    "non_standard_excel": "/home/gzy/rag-biomap/dataset/å¯¼å‡ºæ•°æ®ç¬¬1~1000æ¡æ•°æ®_ç—…æ¡ˆé¦–é¡µ-.xlsx",
    "standard_excel": "/home/gzy/rag-biomap/dataset/VTE-PTE-CTEPHç ”ç©¶æ•°æ®åº“.xlsx",
    "header_csv": "/home/gzy/rag-biomap/data_description/test/header_row.csv",
    "standard_terms_csv": "/home/gzy/rag-biomap/data_description/test/æ ‡å‡†æœ¯è¯­_ç—…æ¡ˆé¦–é¡µ.csv",
    "header_vectors": "/home/gzy/rag-biomap/Build_an_index/test/header_terms.npy",
    "standard_vectors": "/home/gzy/rag-biomap/Build_an_index/test/standard_terms.npy",
    "output_excel": "/home/gzy/rag-biomap/dataset/åŒ¹é…ç»“æœå¯¹æ¯”.xlsx",
}

# åç»­å‡½æ•°ä¿æŒä¸å˜ï¼Œç›´æ¥ç”¨ä½ ä¹‹å‰å†™çš„å³å¯

def initialize_directories():
    for path in [CONFIG["header_csv"], CONFIG["header_vectors"]]:
        os.makedirs(os.path.dirname(path), exist_ok=True)

def process_non_standard_data() -> List[str]:
    # å…ˆè°ƒç”¨ä½ çš„ extract_first_row_to_csvï¼Œç¡®ä¿ CSV ç”ŸæˆæˆåŠŸ
    if not extract_first_row_to_csv(CONFIG["non_standard_excel"], CONFIG["header_csv"]):
        raise RuntimeError("éæ ‡å‡†æ•°æ®å¤„ç†å¤±è´¥")

    # è°ƒç”¨å°è£…å¥½çš„å‘é‡åŒ–å‡½æ•°ï¼Œæ›¿ä»£åŸæ¥çš„ build_index_from_csv è°ƒç”¨
    vectorize_header_terms(
        CONFIG["header_csv"],
        CONFIG["header_vectors"],
        failed_log_path="/home/gzy/rag-biomap/Build_an_index/test/header_terms_failed.csv"
    )

    # è¿”å›æ‰€æœ‰æ–‡æœ¬åˆ—è¡¨
    return pd.read_csv(CONFIG["header_csv"], header=None)[0].tolist()

def process_standard_data() -> List[str]:
    success = extract_name_columns_from_excel(
        CONFIG["standard_excel"],
        CONFIG["standard_terms_csv"],
        target_sheet="ç—…æ¡ˆé¦–é¡µä¿¡æ¯",
        target_column="åç§°"
    )
    if not success:
        raise RuntimeError("æ ‡å‡†æœ¯è¯­æå–å¤±è´¥")

    df = pd.read_csv(CONFIG["standard_terms_csv"])
    required_columns = ["sheetåç§°", "å†…å®¹"]
    if not all(col in df.columns for col in required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        raise ValueError(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: {missing}")

    target_sheet = "ç—…æ¡ˆé¦–é¡µä¿¡æ¯"
    df_filtered = df[df["sheetåç§°"] == target_sheet]
    if df_filtered.empty:
        raise ValueError(f"æœªæ‰¾åˆ°å·¥ä½œè¡¨: {target_sheet}")

    terms = df_filtered["å†…å®¹"].dropna().astype(str).tolist()

    new_csv_path = "/home/gzy/rag-biomap/data_description/test/ç—…æ¡ˆé¦–é¡µä¿¡æ¯_å†…å®¹.csv"
    df_filtered[["å†…å®¹"]].to_csv(new_csv_path, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²ä¿å­˜ç­›é€‰åå†…å®¹åˆ°ï¼š{new_csv_path}ï¼Œå…± {len(terms)} æ¡")

    build_index_from_csv(
        new_csv_path,
        CONFIG["standard_vectors"],
        column_index=0,
        verbose=False
    )
    return terms

def generate_with_llm(prompt: str) -> str:
    try:
        response = client.chat.completions.create(
            model="CAIRI-LLM",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=100
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ï¼š{e}")
        return "[é»˜è®¤å›å¤]"


"""ä½™å¼¦
def calculate_similarities() -> List[Dict]:
   header_vectors = np.load(CONFIG["header_vectors"])
    standard_vectors = np.load(CONFIG["standard_vectors"])
    header_texts = pd.read_csv(CONFIG["header_csv"], header=None)[0].tolist()
    standard_texts = pd.read_csv(CONFIG["standard_terms_csv"])["å†…å®¹"].tolist()

    results = []
    for h_text, h_vec in zip(header_texts, header_vectors):
        sim_scores = cosine_similarity([h_vec], standard_vectors)[0]
        top_3_indices = np.argsort(sim_scores)[-3:][::-1]
        top_3 = [standard_texts[i] for i in top_3_indices]
        top_scores = [sim_scores[i] for i in top_3_indices]

        """
#bm25
def calculate_similarities() -> List[Dict]:
    header_texts = pd.read_csv(CONFIG["header_csv"], header=None)[0].dropna().astype(str).tolist()
    standard_texts = pd.read_csv(CONFIG["standard_terms_csv"])["å†…å®¹"].dropna().astype(str).tolist()

    tokenized_corpus = [list(jieba.cut(text)) for text in standard_texts]
    bm25 = BM25Okapi(tokenized_corpus)

    results = []
    for h_text in header_texts:
        query = list(jieba.cut(h_text))
        scores = bm25.get_scores(query)
        top_3_indices = np.argsort(scores)[-3:][::-1]
        top_3 = [standard_texts[i] for i in top_3_indices]
        top_scores = [scores[i] for i in top_3_indices]

        prompt = f"""è¯·æ ¹æ®ç—…å†è¡¨å¤´é€‰æ‹©æœ€åŒ¹é…çš„æ ‡å‡†æœ¯è¯­ï¼š
åŸå§‹è¡¨å¤´ï¼š{h_text}
å€™é€‰æœ¯è¯­ï¼š
{chr(10).join(f'{i + 1}. {text}' for i, text in enumerate(top_3))}

åªéœ€è¿”å›é€‰æ‹©çš„ç¼–å·(1-3)ï¼Œä¸è¦è§£é‡Šã€‚"""

        llm_choice = generate_with_llm(prompt)
        results.append({
            "åŸå§‹è¡¨å¤´": h_text,
            "å€™é€‰æœ¯è¯­": top_3,
            "LLMé€‰æ‹©": top_3[int(llm_choice) - 1] if llm_choice.isdigit() else "N/A",
            "æœ€é«˜ç›¸ä¼¼åº¦": top_scores[0],
            "å¹³å‡ç›¸ä¼¼åº¦": np.mean(top_scores)
        })
    return results

def save_results(results: List[Dict]):
    df = pd.DataFrame(results)
    df['åŒ¹é…æˆåŠŸ'] = df.apply(lambda x: x['LLMé€‰æ‹©'] in x['å€™é€‰æœ¯è¯­'][0], axis=1)
    df.to_excel(CONFIG["output_excel"], index=False, engine='openpyxl')
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {CONFIG['output_excel']}ï¼Œå…± {len(df)} æ¡è®°å½•")

def main():
    initialize_directories()
    print("ğŸ”„ å¤„ç†éæ ‡å‡†æ•°æ®...")
    header_texts = process_non_standard_data()

    print("ğŸ”„ å¤„ç†æ ‡å‡†çŸ¥è¯†åº“...")
    standard_texts = process_standard_data()

    print("ğŸ” è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ‰§è¡Œç®€åŒ–RAGæµç¨‹...")
    results = calculate_similarities()

    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_results(results)

if __name__ == "__main__":
    main()
