import pandas as pd
import numpy as np
import requests
from sklearn.metrics.pairwise import cosine_similarity
from data_description.invoke_Non_standard_data import extract_first_row_to_csv
from data_description.invoke_data_manipulaltion import extract_name_columns_from_excel
from Build_an_index.invoke_Build_index import get_embedding, build_index_from_csv
import os
from typing import List, Dict

# é…ç½®å‚æ•°
CONFIG = {
    # æ–‡ä»¶è·¯å¾„
    "non_standard_excel": "/home/gzy/rag-biomap/å¯¼å‡ºæ•°æ®ç¬¬1~1000æ¡æ•°æ®_ç—…æ¡ˆé¦–é¡µ-.xlsx",
    "standard_excel": "/home/gzy/rag-biomap/VTE-PTE-CTEPHç ”ç©¶æ•°æ®åº“.xlsx",
    "header_csv": "/home/gzy/rag-biomap/data_description/header_row.csv",
    "standard_terms_csv": "/home/gzy/rag-biomap/data_description/æ ‡å‡†æœ¯è¯­åˆå¹¶ç»“æœ.csv",
    "header_vectors": "/home/gzy/rag-biomap/Build_an_index/header_terms.npy",
    "standard_vectors": "/home/gzy/rag-biomap/Build_an_index/standard_terms.npy",
    "output_excel": "/home/gzy/rag-biomap/åŒ¹é…ç»“æœå¯¹æ¯”.xlsx",

    # æ¨¡å‹é…ç½®
    "embedding_model": "nomic-embed-text",  # åµŒå…¥æ¨¡å‹
    "llm_api": {
        "base_url": "http://172.16.55.171:7010/v1",
        "api_key": "sk-cairi",
        "model": "gpt-4"  # å‡è®¾ä½¿ç”¨çš„ç”Ÿæˆæ¨¡å‹
    }
}


def initialize_directories():
    """ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨"""
    for path in [CONFIG["header_csv"], CONFIG["header_vectors"]]:
        os.makedirs(os.path.dirname(path), exist_ok=True)


def process_non_standard_data() -> List[str]:
    """å¤„ç†éæ ‡å‡†æ•°æ®å¹¶è¿”å›è¡¨å¤´æ–‡æœ¬åˆ—è¡¨"""
    if not extract_first_row_to_csv(CONFIG["non_standard_excel"], CONFIG["header_csv"]):
        raise RuntimeError("éæ ‡å‡†æ•°æ®å¤„ç†å¤±è´¥")
    build_index_from_csv(CONFIG["header_csv"], CONFIG["header_vectors"], column_index=0, verbose=False)
    return pd.read_csv(CONFIG["header_csv"], header=None)[0].tolist()


def process_standard_data() -> List[str]:
    """å¤„ç†æ ‡å‡†çŸ¥è¯†åº“æ•°æ®å¹¶è¿”å›æœ¯è¯­åˆ—è¡¨"""
    extract_name_columns_from_excel()

    # è¯»å–CSVå¹¶éªŒè¯åˆ—å
    df = pd.read_csv(CONFIG["standard_terms_csv"])
    required_columns = ["sheetåç§°", "å†…å®¹"]
    if not all(col in df.columns for col in required_columns):
        missing = [col for col in required_columns if col not in df.columns]
        raise ValueError(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: {missing}")

    # è¿‡æ»¤ç‰¹å®šsheetå†…å®¹
    target_sheet = "ç—…æ¡ˆé¦–é¡µä¿¡æ¯"
    df = df[df["sheetåç§°"] == target_sheet]
    if df.empty:
        raise ValueError(f"æœªæ‰¾åˆ°å·¥ä½œè¡¨: {target_sheet}")

    # æå–ç¬¬ä¸‰éƒ¨åˆ†å†…å®¹
    terms = df["å†…å®¹"].dropna().astype(str).tolist()

    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    temp_csv = "/tmp/ç—…æ¡ˆé¦–é¡µä¿¡æ¯_ç¬¬ä¸‰éƒ¨åˆ†.csv"
    pd.DataFrame({"term": terms}).to_csv(temp_csv, index=False)

    # ç”Ÿæˆå‘é‡
    build_index_from_csv(
        temp_csv,
        CONFIG["standard_vectors"],
        column_index=0,
        verbose=False  # å…³é—­è¯¦ç»†è¾“å‡º
    )
    return terms


def generate_with_llm(prompt: str) -> str:
    """è°ƒç”¨LLM APIç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    try:
        response = requests.post(
            f"{CONFIG['llm_api']['base_url']}/chat/completions",
            headers={"Authorization": f"Bearer {CONFIG['llm_api']['api_key']}"},
            json={
                "model": CONFIG["llm_api"]["model"],
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        print(f"âš ï¸ LLMç”Ÿæˆå¤±è´¥: {str(e)}")
        return "N/A"


def calculate_similarities() -> List[Dict]:
    """ç®€åŒ–åçš„RAGæµç¨‹ï¼Œå»æ‰é‡æ’éƒ¨åˆ†"""
    # åŠ è½½æ•°æ®
    header_vectors = np.load(CONFIG["header_vectors"])
    standard_vectors = np.load(CONFIG["standard_vectors"])
    header_texts = pd.read_csv(CONFIG["header_csv"], header=None)[0].tolist()
    standard_texts = pd.read_csv(CONFIG["standard_terms_csv"])["å†…å®¹"].tolist()

    results = []
    for h_text, h_vec in zip(header_texts, header_vectors):
        # 1. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim_scores = cosine_similarity([h_vec], standard_vectors)[0]
        top_3_indices = np.argsort(sim_scores)[-3:][::-1]  # ç›´æ¥å–ç›¸ä¼¼åº¦æœ€é«˜çš„3ä¸ª
        top_3 = [standard_texts[i] for i in top_3_indices]
        top_scores = [sim_scores[i] for i in top_3_indices]

        # 2. LLMç”Ÿæˆæœ€ç»ˆé€‰æ‹©
        prompt = f"""è¯·æ ¹æ®ç—…å†è¡¨å¤´é€‰æ‹©æœ€åŒ¹é…çš„æ ‡å‡†æœ¯è¯­ï¼š

        åŸå§‹è¡¨å¤´ï¼š{h_text}
        å€™é€‰æœ¯è¯­ï¼š
        {chr(10).join(f'{i + 1}. {text}' for i, text in enumerate(top_3))}

        åªéœ€è¿”å›é€‰æ‹©çš„ç¼–å·(1-3)ï¼Œä¸è¦è§£é‡Šã€‚"""

        llm_choice = generate_with_llm(prompt)
        results.append({
            "åŸå§‹è¡¨å¤´": h_text,
            "å€™é€‰æœ¯è¯­": top_3,
            "LLMé€‰æ‹©": top_3[int(llm_choice) - 1] if llm_choice.isdigit() else "N/A",
            "æœ€é«˜ç›¸ä¼¼åº¦": top_scores[0],
            "å¹³å‡ç›¸ä¼¼åº¦": np.mean(top_scores)
        })
    return results


def save_results(results: List[Dict]):
    """ä¿å­˜å¸¦æœ‰å¤šç»´åº¦è¯„ä¼°çš„ç»“æœ"""
    df = pd.DataFrame(results)
    # æ·»åŠ åŒ¹é…æˆåŠŸæ ‡è®°
    df['åŒ¹é…æˆåŠŸ'] = df.apply(
        lambda x: x['LLMé€‰æ‹©'] in x['å€™é€‰æœ¯è¯­'][0],
        axis=1
    )
    df.to_excel(CONFIG["output_excel"], index=False, engine='openpyxl')
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {CONFIG['output_excel']}ï¼Œå…± {len(df)} æ¡è®°å½•")


def main():
    initialize_directories()
    print("ğŸ”„ å¤„ç†éæ ‡å‡†æ•°æ®...")
    header_texts = process_non_standard_data()

    print("ğŸ”„ å¤„ç†æ ‡å‡†çŸ¥è¯†åº“...")
    standard_texts = process_standard_data()

    print("ğŸ” è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ‰§è¡Œç®€åŒ–RAGæµç¨‹...")
    results = calculate_similarities()

    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_results(results)


if __name__ == "__main__":
    main()