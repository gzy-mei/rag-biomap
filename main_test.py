import sys
import os
import re
import argparse
import pandas as pd
import numpy as np
import jieba
import tqdm
import json
from openai import OpenAI
from typing import List, Dict
#ËøõË°åÊï∞ÊçÆÈõÜÂ§ÑÁêÜ
from data_description.invoke_data_manipulaltion import extract_name_columns_from_multiple_sheets
from data_description.invoke_Non_standard_data import extract_first_row_to_csv
#ËøõË°åÂêëÈáèÂåñ
from Build_an_index.invoke_Build_index import get_embedding, build_index_from_csv
from Build_an_index.invoke_Non_standard_data_Build_index import vectorize_header_terms
#ËÆ°ÁÆóÂêëÈáèÁõ∏‰ººÂ∫¶
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi
#ËøõÂ∫¶Êù°Â±ïÁ§∫
from tqdm import tqdm
# Áî®‰∫éÂÆûÁé∞Â§öÁ∫øÁ®ãÂπ∂ÂèëÂ§ÑÁêÜ„ÄÇ
from concurrent.futures import ThreadPoolExecutor, as_completed

# ÂàùÂßãÂåñOpenAIÂÆ¢Êà∑Á´Ø
client = OpenAI(
    base_url="http://172.16.55.171:7010/v1",
    #base_url="http://10.0.1.194:7010/v1",
    api_key="sk-cairi"
)

# ÈÖçÁΩÆÂèÇÊï∞
CONFIG = {
    "llm_model": "CAIRI-LLM-reasoner",
    "non_standard_excel": "dataset/ÂØºÂá∫Êï∞ÊçÆÁ¨¨1~1000Êù°Êï∞ÊçÆ_ÁóÖÊ°àÈ¶ñÈ°µ-.xlsx",
    "standard_excel": "dataset/VTE-PTE-CTEPHÁ†îÁ©∂Êï∞ÊçÆÂ∫ì.xlsx",
    "header_csv": "data_description/test/header_row.csv",
    "standard_terms_csv": "data_description/test/Ê†áÂáÜÊúØËØ≠_ÁóÖÊ°àÈ¶ñÈ°µ.csv",
    "header_vectors": "Build_an_index/test/header_terms.npy",
    "standard_vectors": "Build_an_index/test/standard_terms.npy",
    "output_excel": "dataset/ÂåπÈÖçÁªìÊûúÂØπÊØî.xlsx",
    "output_dir": "dataset/Matching_Results_Comparison"
}

#ÂàùÂßãÂåñÁõÆÂΩïÁªìÊûÑÔºö Ê†πÊçÆÈÖçÁΩÆÊñá‰ª∂CONFIG‰∏≠ÁöÑË∑ØÂæÑÔºåËá™Âä®ÂàõÂª∫Ëøô‰∫õË∑ØÂæÑÊâÄÂú®ÁöÑÊñá‰ª∂Â§πÁõÆÂΩïÔºàÂ¶ÇÊûúÁõÆÂΩï‰∏çÂ≠òÂú®Êª¥ËØùÔºâ
def initialize_directories():
    for path in [CONFIG["header_csv"], CONFIG["header_vectors"]]:
        os.makedirs(os.path.dirname(path), exist_ok=True)

#Â§ÑÁêÜÈùûÊ†áÂáÜÊï∞ÊçÆÔºàË°®Â§¥‰ø°ÊÅØÔºâ
def process_non_standard_data() -> List[str]:
    #from data_description.invoke_Non_standard_data import extract_first_row_to_csv
    # ÂÖàË∞ÉÁî®extract_first_row_to_csvÔºåÁ°Æ‰øùCSVÁîüÊàêÊàêÂäü
    #ÁªìÊûúÁîüÊàêÔºödata_description/test/header_row.csv
    if not extract_first_row_to_csv(CONFIG["non_standard_excel"], CONFIG["header_csv"]):
        raise RuntimeError("ÈùûÊ†áÂáÜÊï∞ÊçÆÂ§ÑÁêÜÂ§±Ë¥•")
    # Ë∞ÉÁî®Â∞ÅË£ÖÂ•ΩÁöÑÂêëÈáèÂåñÂáΩÊï∞
    #from Build_an_index.invoke_Non_standard_data_Build_index import vectorize_header_terms
    #ÁªìÊûúÁîüÊàêÔºöBuild_an_index/test/header_terms.npy
    vectorize_header_terms(
        CONFIG["header_csv"],
        CONFIG["header_vectors"],
        failed_log_path="Build_an_index/test/header_terms_failed.csv"
    )
    # ËøîÂõûÊâÄÊúâÊñáÊú¨ÂàóË°®
    return pd.read_csv(CONFIG["header_csv"], header=None)[0].tolist()

#Â§ÑÁêÜÊ†áÂáÜÊúØËØ≠Êï∞ÊçÆÔºàÁü•ËØÜÂ∫ìÔºâ
def process_standard_data() -> List[str]:
    """
    Â§ÑÁêÜÊ†áÂáÜÊúØËØ≠Êï∞ÊçÆÔºöÊèêÂèñÂ§ö‰∏™ sheet ÁöÑ‚ÄúÂêçÁß∞‚ÄùÂàó ‚Üí ‰øùÂ≠òCSV ‚Üí ÂêëÈáèÂåñ ‚Üí ËøîÂõûÊúØËØ≠ÂàóË°®
    """
    # ‚úÖ ÊèêÂèñÂ§ö‰∏™ sheet ÁöÑ‚ÄúÂêçÁß∞‚ÄùÂàó
    target_sheets = ["ÊÇ£ËÄÖÂü∫Á∫ø‰ø°ÊÅØ", "ÁóÖÊ°àÈ¶ñÈ°µ‰ø°ÊÅØ"]
    success = extract_name_columns_from_multiple_sheets(
        CONFIG["standard_excel"],
        CONFIG["standard_terms_csv"],
        target_sheets=target_sheets,
        target_column="ÂêçÁß∞"
    )
    if not success:
        raise RuntimeError("Ê†áÂáÜÊúØËØ≠ÊèêÂèñÂ§±Ë¥•")

    # ‚úÖ Âä†ËΩΩCSVÂπ∂ÊèêÂèñÊúØËØ≠ÔºàÂç≥‚ÄúÂÜÖÂÆπ‚ÄùÂàóÔºâ
    df = pd.read_csv(CONFIG["standard_terms_csv"])
    if "ÂÜÖÂÆπ" not in df.columns:
        raise ValueError("Ê†áÂáÜÊúØËØ≠CSVÁº∫Â∞ë 'ÂÜÖÂÆπ' Âàó")
    terms = df["ÂÜÖÂÆπ"].dropna().astype(str).tolist()
    print(f"‚úÖ ÊàêÂäüÂä†ËΩΩÊ†áÂáÜÊúØËØ≠ÔºåÂÖ± {len(terms)} Êù°")

    # ‚úÖ ÂêëÈáèÂåñ‚ÄúÂÜÖÂÆπ‚ÄùÂàó
    build_index_from_csv(
        CONFIG["standard_terms_csv"],     # Áõ¥Êé•‰ΩøÁî®ÂéüCSV
        CONFIG["standard_vectors"],       # ‰øùÂ≠òÂêëÈáèË∑ØÂæÑ
        column_index=2,                   # ‚ÄúÂÜÖÂÆπ‚ÄùÂàóÂú®CSV‰∏≠ÁöÑ‰ΩçÁΩÆ
        verbose=False
    )
    return terms


def detect_similarity_method(func):
    def wrapper(*args, **kwargs):
        method_name = func.__name__.lower()
        if "bm25" in method_name:
            CONFIG["similarity_method"] = "BM25"
        elif "cosine" in method_name:
            CONFIG["similarity_method"] = "Cosine"
        else:
            CONFIG["similarity_method"] = "Unknown"
        return func(*args, **kwargs)
    return wrapper


prompt_template = r"""
‰Ω†ÊòØÂåªÁñóÊï∞ÊçÆÊ†áÂáÜÂåñÂä©Êâã„ÄÇËØ∑‰ªé‰∏ãÂàóÂÄôÈÄâÊúØËØ≠‰∏≠ÔºåÈÄâÊã©‰∏éÁªôÂÆöÂéüÂßãË°®Â§¥ÊúÄÂåπÈÖçÁöÑ‰∏Ä‰∏™„ÄÇ

ÂåπÈÖçËßÑÂàôÔºö
1. ÂÆåÂÖ®‰∏ÄËá¥‰ºòÂÖàÔºõ
2. ÂøΩÁï•Êó†ÊÑè‰πâËØçÔºàÂ¶Ç‚ÄúÂá∫Èô¢‚Äù„ÄÅ‚ÄúÁºñÂè∑‚Äù„ÄÅ‚Äú3‚ÄùÁ≠âÔºâÔºå‰ΩÜ‰øùÁïôÂ¶Ç‚ÄúÂÖ•Èô¢‚Äù‚ÄúÁóÖÊÉÖ‚ÄùÔºõ
3. Ëã•Êó†ÂêàÈÄÇÂåπÈÖçÔºåËøîÂõû N/A Âíå 0.0 ÂàÜÔºõ
4. ÂàÜÊï∞‰∏∫ (0, 1.0]ÔºåÂåπÈÖçË∂äÊé•ËøëÔºåÂàÜÊï∞Ë∂äÈ´òÔºõ

‚ö†Ô∏è„ÄêËæìÂá∫Ë¶ÅÊ±Ç„Äë
‰∏•Ê†º‰ªÖËæìÂá∫Â¶Ç‰∏ãÊ†ºÂºèÔºå‰∏çËÉΩÊúâËß£Èáä„ÄÅ‰ª£Á†ÅÂùó„ÄÅÊç¢Ë°åÊàñÂÖ∂ÂÆÉÂÜÖÂÆπÔºö
{"matched_field_name": "xxx", "score": x.x}

---

ÂéüÂßãË°®Â§¥: {{h_text}}
ÂÄôÈÄâÊúØËØ≠: {{top_3}}
ËØ∑ËæìÂá∫Ôºö
"""


def generate_with_llm(prompt: str) -> str:
    try:
        response = client.chat.completions.create(
            model=CONFIG["llm_model"],
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            presence_penalty=1.5,
            extra_body={"min_p": 0},
        )

        message_obj = response.choices[0].message
        raw_content = None
        if hasattr(message_obj, "content") and message_obj.content:
            raw_content = message_obj.content.strip()
        elif hasattr(message_obj, "reasoning_content") and message_obj.reasoning_content:
            raw_content = message_obj.reasoning_content.strip()
        else:
            raw_content = ""

        # ‚úÖ Âü∫Á°ÄÊ∏ÖÊ¥óÔºöÂéªÈô§ markdown JSON ÂåÖË£π
        if raw_content.startswith("```json"):
            raw_content = re.sub(r"^```json", "", raw_content).strip()
            raw_content = re.sub(r"```$", "", raw_content).strip()
        elif raw_content.startswith("```"):
            raw_content = re.sub(r"^```", "", raw_content).strip()
            raw_content = re.sub(r"```$", "", raw_content).strip()

        # ‚úÖ Êâ©Â±ïÊ∏ÖÊ¥óÔºö‰∏≠ÊñáÁ¨¶Âè∑ + ÊãºÂÜô‰øÆÂ§ç + ÂÜó‰ΩôÂ≠óÊÆµ
        raw_content = raw_content.replace("‚Äú", "\"").replace("‚Äù", "\"")
        raw_content = raw_content.replace("Ôºå", ",").replace("Ôºö", ":")
        raw_content = raw_content.replace("matchee", "matched_field_name")
        if '"matched_field_' in raw_content:
            raw_content = raw_content.replace('"matched_field_"', '"matched_field_name"')
        raw_content = raw_content.rstrip("„ÄÇ")

        # ‚úÖ Ëá™Âä®Ë°•ÂÖ®Áº∫Â§±ÁöÑÂ§ßÊã¨Âè∑
        if raw_content.count("{") > raw_content.count("}"):
            raw_content += "}"
        elif raw_content.count("{") < raw_content.count("}"):
            raw_content = raw_content[:raw_content.rfind("}")+1]

        # ‚úÖ Ê≠£ÂàôÊèêÂèñ JSON ‰∏ª‰Ωì
        # ‚úÖ Êõ¥‰∏•Ê†ºÁöÑÊ≠£ÂàôÊèêÂèñ JSON ‰∏ª‰ΩìÔºåÁ°Æ‰øùÂè™ÊèêÂèñ‰∏ÄÊÆµ
        try:
            json_match = re.findall(r"\{.*?\}", raw_content.strip(), re.DOTALL)
            if len(json_match) != 1:
                print(f"‚ö†Ô∏è ËøîÂõû‰∫Ü {len(json_match)} ÊÆµ JSONÔºåÊó†Ê≥ïÂà§Êñ≠‰ΩøÁî®Âì™ÊÆµ")
                print(f"PromptÔºö{prompt}")
                print(f"ÂéüÂßãËøîÂõûÂÜÖÂÆπÔºö{raw_content}")
                return "Ë∞ÉÁî®Â§±Ë¥•"

            parsed = json.loads(json_match[0])
            matched = parsed.get("matched_field_name", "")
            if matched == "N/A":
                return ""
            return matched

            parsed = json.loads(raw_content)
            matched = parsed.get("matched_field_name", "")
            if matched == "N/A":
                return ""
            return matched

        except Exception as e:


            print("‚ö†Ô∏è JSONËß£ÊûêÂ§±Ë¥•ÔºÅ")
            print(f"PromptÔºö{prompt}")
            print(f"ËøîÂõûÂéüÊñáÔºö{raw_content}")
            print(f"ÂºÇÂ∏∏‰ø°ÊÅØÔºö{e}")
            return "Ë∞ÉÁî®Â§±Ë¥•"

    except Exception as e:
        print(f"‚ö†Ô∏è LLMË∞ÉÁî®Â§±Ë¥•Ôºö{e}")
        return "Ë∞ÉÁî®Â§±Ë¥•"


# threshold_ratio = 0.34
@detect_similarity_method
def calculate_similarities_bm25() -> List[Dict]:
    # === Âä†ËΩΩÊï∞ÊçÆ ===
    header_texts = pd.read_csv(CONFIG["header_csv"], header=None)[0].dropna().astype(str).tolist()
    standard_texts = pd.read_csv(CONFIG["standard_terms_csv"])["ÂÜÖÂÆπ"].dropna().astype(str).tolist()

    # === ÊûÑÂª∫ BM25 Á¥¢Âºï ===
    tokenized_corpus = [list(jieba.cut(text)) for text in standard_texts]
    bm25 = BM25Okapi(tokenized_corpus)

    # === È¢ÑËÆ°ÁÆóÊâÄÊúâË°®Â§¥ÁöÑÂæóÂàÜ ===
    header_scores_list = []
    for h_text in header_texts:
        query = list(jieba.cut(h_text))
        scores = bm25.get_scores(query)
        header_scores_list.append(scores)

    # === ÊèêÂèñÊØè‰∏™Ë°®Â§¥ÁöÑÊúÄÈ´òÂæóÂàÜÂπ∂Áªü‰∏ÄËÆ°ÁÆó‰∏≠‰ΩçÊï∞ ===
    max_scores = [max(scores) for scores in header_scores_list]
    median_global_score = np.median(max_scores)
    print(f"üìè ÂÖ®Â±Ä‰∏≠‰ΩçÊï∞ÈòàÂÄº‰∏∫Ôºö{median_global_score:.4f}")

    # === Âçï‰∏™Ë°®Â§¥Â§ÑÁêÜÂáΩÊï∞ ===
    def process_single_header(index: int, h_text: str) -> Dict:
        scores = header_scores_list[index]
        top_3_indices = np.argsort(scores)[-3:][::-1]
        top_3 = [standard_texts[i] for i in top_3_indices]
        top_score = scores[top_3_indices[0]]
        relative_score = round(top_score / max(max_scores), 4)

        # === ‰∏çË∞ÉÁî® LLM ÁöÑÊù°‰ª∂ ===
        if top_score < median_global_score:
            return {
                "ÂéüÂßãË°®Â§¥": h_text,
                "ÂÄôÈÄâÊúØËØ≠": top_3,
                "LLMÈÄâÊã©": "",
                "ÊúÄÈ´òÁõ∏‰ººÂ∫¶": round(top_score, 4),
                "ÊúÄÈ´òÂàÜÁõ∏ÂØπÊØî‰æãÔºàÂΩìÂâç/maxÔºâ": relative_score,
                "ÊòØÂê¶Ë∞ÉÁî®LLM": "Âê¶"
            }

        # === ÊûÑÈÄ† prompt Âπ∂Ë∞ÉÁî® LLM ===
        prompt = prompt_template.replace("{{h_text}}", h_text).replace("{{top_3}}", json.dumps(top_3, ensure_ascii=False))
        llm_choice_result = generate_with_llm(prompt)

        final_choice = "" if llm_choice_result == "Ë∞ÉÁî®Â§±Ë¥•" else llm_choice_result.strip()

        return {
            "ÂéüÂßãË°®Â§¥": h_text,
            "ÂÄôÈÄâÊúØËØ≠": top_3,
            "LLMÈÄâÊã©": final_choice,
            "ÊúÄÈ´òÁõ∏‰ººÂ∫¶": round(top_score, 4),
            "ÊúÄÈ´òÂàÜÁõ∏ÂØπÊØî‰æãÔºàÂΩìÂâç/maxÔºâ": relative_score,
            "ÊòØÂê¶Ë∞ÉÁî®LLM": "ÊòØ"
        }

    # === Â§öÁ∫øÁ®ãÂ§ÑÁêÜ ===
    results = [None] * len(header_texts)

    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {
            executor.submit(process_single_header, idx, h_text): idx
            for idx, h_text in enumerate(header_texts)
        }

        with tqdm(total=len(header_texts), desc="üß† LLMÂåπÈÖç‰∏≠", ncols=80) as pbar:
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    result = future.result()
                    results[idx] = result
                except Exception as e:
                    print(f"‚ö†Ô∏è Ë°®Â§¥Â§ÑÁêÜÂ§±Ë¥•Ôºàindex={idx}Ôºâ: {e}")
                finally:
                    pbar.update(1)

    return results


def save_results(results: List[Dict]):
    results = [r for r in results if r is not None]
    df = pd.DataFrame(results)
    print("‰øùÂ≠òÂâçÁöÑÂàóÂêçÔºö", df.columns.tolist())

    # Âà†Èô§‰∏çÈúÄË¶ÅÁöÑÂàó
    df.drop(columns=[col for col in ["Âπ≥ÂùáÁõ∏‰ººÂ∫¶", "ÂåπÈÖçÊàêÂäü"] if col in df.columns], inplace=True)

    # Âä†ËΩΩ GT Ê†áÂáÜÁ≠îÊ°àÔºàË∑≥ËøáË°®Â§¥Ôºâ
    gt_path = "/home/gzy/rag-biomap/dataset/GT.xlsx"
    gt_df = pd.read_excel(gt_path, header=0)

    if gt_df.shape[1] < 2:
        raise ValueError("GT.xlsx ÂøÖÈ°ªËá≥Â∞ëÂåÖÂê´‰∏§ÂàóÔºåÁ¨¨‰∫åÂàó‰∏∫Ê†áÂáÜÁ≠îÊ°à")

    gt_answers = gt_df.iloc[:, 1].fillna("").astype(str).tolist()
    df["GTÊ†áÂáÜÁ≠îÊ°à"] = pd.Series(gt_answers[:len(df)])

    # ÂåπÈÖçÂà§Êñ≠
    # df["ÊòØÂê¶ÂåπÈÖçGT"] = df.apply(lambda row: row["LLMÈÄâÊã©"].strip() == row["GTÊ†áÂáÜÁ≠îÊ°à"].strip(), axis=1)

    def normalize_text(s):
        if not isinstance(s, str):
            return ""
        # ÂéªÈô§Á©∫Ê†º„ÄÅ‰∏≠ÊñáËã±ÊñáÂÜíÂè∑„ÄÅÂè•Âè∑„ÄÅÊç¢Ë°åÁ≠âÂ∑ÆÂºÇ
        s = s.strip()
        s = s.replace("Ôºö", ":")  # ‰∏≠ÊñáÂÜíÂè∑Êç¢ÊàêËã±Êñá
        s = s.replace("„ÄÇ", ".")
        s = s.replace("\n", "")
        s = s.replace("\r", "")
        s = s.replace(" ", "")
        return s

    df["ÊòØÂê¶ÂåπÈÖçGT"] = df.apply(
        lambda row: normalize_text(row["LLMÈÄâÊã©"]) == normalize_text(row["GTÊ†áÂáÜÁ≠îÊ°à"]),
        axis=1
    )


    # ÁªüËÆ°‰ø°ÊÅØ
    total_accuracy = df["ÊòØÂê¶ÂåπÈÖçGT"].mean()
    gt_empty_count = sum(df["GTÊ†áÂáÜÁ≠îÊ°à"] == "")
    llm_empty = df["LLMÈÄâÊã©"] == ""
    gt_empty = df["GTÊ†áÂáÜÁ≠îÊ°à"] == ""
    llm_not_empty = df["LLMÈÄâÊã©"] != ""

    llm_empty_and_gt_empty = df[llm_empty & gt_empty].shape[0]
    llm_empty_total = llm_empty.sum()
    llm_not_empty_total = llm_not_empty.sum()
    llm_not_empty_gt_empty = df[llm_not_empty & gt_empty].shape[0]
    llm_empty_gt_not_empty = df[llm_empty & ~gt_empty].shape[0]

    # ÂàõÂª∫ÁªüËÆ°‰ø°ÊÅØDataFrame
    stats_data = {
        "ÁªüËÆ°ÊåáÊ†á": [
            "llmÈÄâÊã©‰∏éGTÊ†áÂáÜÁ≠îÊ°àÂåπÈÖçÂáÜÁ°ÆÁéá",
            "GTÊ†áÂáÜÁ≠îÊ°à‰∏≠Á©∫ÂÄº‰∏™Êï∞",
            "llmÈÄâÊã©‰∏∫Á©∫ÔºåGT‰πü‰∏∫Á©∫ÁöÑÂåπÈÖçÊàêÂäüÊï∞Èáè",
            "llmÈÄâÊã©‰∏∫Á©∫ÁöÑÊï∞Èáè",
            "llmÈÄâÊã©ÈùûÁ©∫ÁöÑÊï∞Èáè",
            "llmÈÄâÊã©ÈùûÁ©∫Ôºå‰ΩÜGTÊòØÁ©∫ÁöÑÊï∞Èáè",
            "llmÈÄâÊã©‰∏∫Á©∫ÔºåGT‰∏ç‰∏∫Á©∫ÁöÑÊï∞Èáè"
        ],
        "Êï∞ÂÄº": [
            total_accuracy,
            gt_empty_count,
            llm_empty_and_gt_empty,
            llm_empty_total,
            llm_not_empty_total,
            llm_not_empty_gt_empty,
            llm_empty_gt_not_empty
        ]
    }
    stats_df = pd.DataFrame(stats_data)

    # Â∞ÜÁªüËÆ°‰ø°ÊÅØÂÜôÂÖ•ExcelÁöÑÁ¨¨12-15ÂàóÔºàL-OÂàóÔºâ
    with pd.ExcelWriter(os.path.join(CONFIG["output_dir"], "‰∏≠‰ΩçÊï∞(3).xlsx"), engine="openpyxl") as writer:
        df.to_excel(writer, sheet_name="ÂåπÈÖçÁªìÊûú", index=False)
        stats_df.to_excel(writer, sheet_name="ÂåπÈÖçÁªìÊûú", startcol=11, startrow=1, index=False, header=False)

    # ÊéßÂà∂Âè∞ËæìÂá∫ÔºàËæÖÂä©Á°ÆËÆ§Ôºâ
    print(f"‚úÖ ÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞ {os.path.join(CONFIG['output_dir'], '‰∏≠‰ΩçÊï∞(3).xlsx')}")
    print(f"üìä ÂåπÈÖçÂáÜÁ°ÆÁéáÔºö{total_accuracy:.6f}")
    print(f"üìä GT‰∏∫Á©∫ÂÄºÔºö{gt_empty_count}ÔºållmÈÄâÊã©‰∏∫Á©∫Êï∞ÈáèÔºö{llm_empty_total}")
    print(f"üìä llmÈÄâÊã©‰∏∫Á©∫ && GT‰∏∫Á©∫ÔºàÂåπÈÖçÔºâÔºö{llm_empty_and_gt_empty}")
    print(f"üìä llmÈÄâÊã©ÈùûÁ©∫ && GT‰∏∫Á©∫Ôºö{llm_not_empty_gt_empty}")
    print(f"üìä llmÈÄâÊã©‰∏∫Á©∫ && GTÈùûÁ©∫Ôºö{llm_empty_gt_not_empty}")
    llm_failed_count = sum(df["LLMÈÄâÊã©"] == "Ë∞ÉÁî®Â§±Ë¥•")
    print(f"‚ùó LLMË∞ÉÁî®Â§±Ë¥•Êï∞ÈáèÔºö{llm_failed_count}")



def main():
    from Build_an_index.invoke_Build_index import get_embedding


    #Ëá™Âä®ËØÜÂà´ÂΩìÂâç‰ΩøÁî®ÁöÑÂµåÂÖ•Ê®°Âûã--ÊñπÊ≥ïÔºöËé∑Âèñget_embeddingÂáΩÊï∞ÁöÑÊù•Ê∫êÊ®°ÂùóË∑ØÂæÑÔºàÊØîÂ¶Ç bge_m3.pyÔºâ
    module_path = get_embedding.__module__

    print(f"üìÅ Ê£ÄÊµãÂà∞ get_embedding Êù•Ëá™Ê®°ÂùóÔºö{module_path}")  # ‚úÖ ÔºöË∞ÉËØïËæìÂá∫

    if "bge_m3" in module_path:
        CONFIG["embedding_model"] = "bge-m3"
    elif "nomic_embed_text" in module_path:
        CONFIG["embedding_model"] = "nomic-embed-text"
    elif "mxbai_embed_large" in module_path:
        CONFIG["embedding_model"] = "mxbai-embed-large"
    else:
        CONFIG["embedding_model"] = "unknown"

    #CONFIG["embedding_model"] = get_embedding.__defaults__[0]

    # Ëá™Âä®Ê£ÄÊµãÂΩìÂâçÂêØÁî®ÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÂáΩÊï∞ÊòØÂì™‰∏Ä‰∏™-ÈÄöËøáÂÖ®Â±ÄÂèòÈáèË°®globals()Êù•Ê£ÄÊµãÂì™‰∏™ÂáΩÊï∞Â≠òÂú®ÔºåÊòØÊàëÊúâÊ≤°Êúâ‚ÄúÂÆö‰πâ‚ÄùÂÆÉ„ÄÇ
    if "calculate_similarities_bm25" in globals():
        calculate_similarities = calculate_similarities_bm25
    elif "calculate_similarities_cosine" in globals():
        calculate_similarities = calculate_similarities_cosine
    else:
        raise ValueError(
            "Êú™Ê£ÄÊµãÂà∞Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÂáΩÊï∞ÔºåËØ∑Á°Æ‰øù‰øùÁïô calculate_similarities_bm25 Êàñ calculate_similarities_cosine ‰∏≠ÁöÑ‰∏Ä‰∏™")

    initialize_directories()
    print("üîÑ Â§ÑÁêÜÈùûÊ†áÂáÜÊï∞ÊçÆ...")
    header_texts = process_non_standard_data()

    print("üîÑ Â§ÑÁêÜÊ†áÂáÜÁü•ËØÜÂ∫ì...")
    standard_texts = process_standard_data()

    print("üîç ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Âπ∂ÊâßË°åÁÆÄÂåñRAGÊµÅÁ®ã...")
    results = calculate_similarities()

#ËøêË°åÁªìÊûúÊòæÁ§∫Ôºöüìê ÂΩìÂâç‰ΩøÁî®ÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊñπÊ≥ï‰∏∫Ôºö......
    print(f"üìê ÂΩìÂâç‰ΩøÁî®ÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÊñπÊ≥ï‰∏∫Ôºö{CONFIG['similarity_method']}")

    print("üíæ ‰øùÂ≠òÁªìÊûú...")
    save_results(results)

if __name__ == "__main__":
    main()
