import sys
import os
import re
import argparse
import pandas as pd
import numpy as np
import jieba
import tqdm
import aiohttp
import asyncio
import json
from openai import OpenAI
from typing import List, Dict
#è¿›è¡Œæ•°æ®é›†å¤„ç†
from data_description.invoke_data_manipulaltion_basyxx import extract_name_columns_from_excel
from data_description.invoke_Non_standard_data import extract_first_row_to_csv
#è¿›è¡Œå‘é‡åŒ–
from Build_an_index.invoke_Build_index import get_embedding, build_index_from_csv
from Build_an_index.invoke_Non_standard_data_Build_index import vectorize_header_terms
#è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi
from tqdm import tqdm

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://172.16.55.171:7010/v1",
    #base_url="http://10.0.1.194:7010/v1",
    api_key="sk-cairi"
)

# é…ç½®å‚æ•°
CONFIG = {
    "llm_model": "CAIRI-LLM-reasoner",
    "non_standard_excel": "dataset/å¯¼å‡ºæ•°æ®ç¬¬1~1000æ¡æ•°æ®_ç—…æ¡ˆé¦–é¡µ-.xlsx",
    "standard_excel": "dataset/VTE-PTE-CTEPHç ”ç©¶æ•°æ®åº“.xlsx",
    "header_csv": "data_description/test/header_row.csv",
    "standard_terms_csv": "data_description/test/æ ‡å‡†æœ¯è¯­_ç—…æ¡ˆé¦–é¡µ.csv",
    "header_vectors": "Build_an_index/test/header_terms.npy",
    "standard_vectors": "Build_an_index/test/standard_terms.npy",
    "output_excel": "dataset/åŒ¹é…ç»“æœå¯¹æ¯”.xlsx",
    #"embedding_model": "nomic-embed-text",  # ä¿®æ”¹ä¸ºå½“å‰ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹åï¼šbge-m3ã€nomic-embed-textã€mxbai-embed-largeè¿˜éœ€è¦å†è°ƒç”¨å‡½æ•°ä¸­ä¿®æ”¹ï¼ï¼ï¼
    #"similarity_method": "BM25",  # ç›¸ä¼¼åº¦æ–¹æ³•ï¼Œæœ‰ï¼šBM25ï¼ŒCosine
    "output_dir": "dataset/Matching_Results_Comparison"

}


#åˆå§‹åŒ–ç›®å½•ç»“æ„ï¼š æ ¹æ®é…ç½®æ–‡ä»¶CONFIGä¸­çš„è·¯å¾„ï¼Œè‡ªåŠ¨åˆ›å»ºè¿™äº›è·¯å¾„æ‰€åœ¨çš„æ–‡ä»¶å¤¹ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨æ»´è¯ï¼‰
def initialize_directories():
    for path in [CONFIG["header_csv"], CONFIG["header_vectors"]]:
        os.makedirs(os.path.dirname(path), exist_ok=True)

#å¤„ç†éæ ‡å‡†æ•°æ®ï¼ˆè¡¨å¤´ä¿¡æ¯ï¼‰
def process_non_standard_data() -> List[str]:
    #from data_description.invoke_Non_standard_data import extract_first_row_to_csv
    # å…ˆè°ƒç”¨extract_first_row_to_csvï¼Œç¡®ä¿CSVç”ŸæˆæˆåŠŸ
    #ç»“æœç”Ÿæˆï¼šdata_description/test/header_row.csv
    if not extract_first_row_to_csv(CONFIG["non_standard_excel"], CONFIG["header_csv"]):
        raise RuntimeError("éæ ‡å‡†æ•°æ®å¤„ç†å¤±è´¥")

    # è°ƒç”¨å°è£…å¥½çš„å‘é‡åŒ–å‡½æ•°
    #from Build_an_index.invoke_Non_standard_data_Build_index import vectorize_header_terms
    #ç»“æœç”Ÿæˆï¼šBuild_an_index/test/header_terms.npy
    vectorize_header_terms(
        CONFIG["header_csv"],
        CONFIG["header_vectors"],
        failed_log_path="Build_an_index/test/header_terms_failed.csv"
    )

    # è¿”å›æ‰€æœ‰æ–‡æœ¬åˆ—è¡¨
    return pd.read_csv(CONFIG["header_csv"], header=None)[0].tolist()

#å¤„ç†æ ‡å‡†æœ¯è¯­æ•°æ®ï¼ˆçŸ¥è¯†åº“ï¼‰
def process_standard_data() -> List[str]:
    """
    å¤„ç†æ ‡å‡†æœ¯è¯­æ•°æ®ï¼šæå–æœ¯è¯­åˆ— â†’ ä¿å­˜CSV â†’ å‘é‡åŒ– â†’ è¿”å›æœ¯è¯­åˆ—è¡¨
    """
    # ç›´æ¥æå–æ ‡å‡†æœ¯è¯­sheetåç§°ã€åŸåˆ—åã€å†…å®¹
    # from data_description.invoke_data_manipulaltion_basyxx import extract_name_columns_from_excel
    #ç»“æœç”Ÿæˆï¼šdata_description/test/æ ‡å‡†æœ¯è¯­_ç—…æ¡ˆé¦–é¡µ.csv
    success = extract_name_columns_from_excel(
        CONFIG["standard_excel"],
        CONFIG["standard_terms_csv"],
        target_sheet="ç—…æ¡ˆé¦–é¡µä¿¡æ¯",
        target_column="åç§°"
    )
    if not success:
        raise RuntimeError("æ ‡å‡†æœ¯è¯­æå–å¤±è´¥")

    # åŠ è½½CSVå¹¶æå–æœ¯è¯­ï¼ˆå³â€œå†…å®¹â€åˆ—-ä¸ä¼šåŒ…å«â€œå†…å®¹â€è¿™ä¸¤ä¸ªå­—ã€‚ï¼‰
    df = pd.read_csv(CONFIG["standard_terms_csv"])
    if "å†…å®¹" not in df.columns:
        raise ValueError("æ ‡å‡†æœ¯è¯­CSVç¼ºå°‘ 'å†…å®¹' åˆ—")

    terms = df["å†…å®¹"].dropna().astype(str).tolist()
    print(f"âœ… æˆåŠŸåŠ è½½æ ‡å‡†æœ¯è¯­ï¼Œå…± {len(terms)} æ¡")

    # å‘é‡åŒ–â€œå†…å®¹â€åˆ—
    #ç»“æœç”Ÿæˆï¼šBuild_an_index/test/standard_terms.npy
    #from Build_an_index.invoke_Build_index import get_embedding, build_index_from_csv
    build_index_from_csv(
        CONFIG["standard_terms_csv"],     # ç›´æ¥ä½¿ç”¨åŸCSV
        CONFIG["standard_vectors"],       # ä¿å­˜å‘é‡è·¯å¾„
        column_index=2,                   # â€œå†…å®¹â€åˆ—åœ¨CSVä¸­çš„ä½ç½®
        verbose=False
    )

    return terms


async def async_generate_with_llm(prompt: str, session: aiohttp.ClientSession) -> str:
    url = "http://172.16.55.171:7010/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer sk-cairi",
        "Content-Type": "application/json"
    }
    payload = {
        "model": CONFIG["llm_model"],
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "presence_penalty": 1.5,
        "extra_body": {"min_p": 0}
    }

    try:
        async with session.post(url, headers=headers, json=payload, timeout=60) as resp:
            if resp.status != 200:
                print(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{resp.status}")
                return ""

            response_json = await resp.json()
            content = response_json.get("choices", [{}])[0].get("message", {}).get("content", "").strip()

            # æå–ç¼–å· 1~3
            match = re.search(r'\b([1-3])\b', content)
            return match.group(1) if match else ""

    except Exception as e:
        print(f"âš ï¸ å¼‚æ­¥è°ƒç”¨å‡ºé”™ï¼š{e}")
        return ""

def detect_similarity_method(func):
    def wrapper(*args, **kwargs):
        method_name = func.__name__.lower()
        if "bm25" in method_name:
            CONFIG["similarity_method"] = "BM25"
        elif "cosine" in method_name:
            CONFIG["similarity_method"] = "Cosine"
        else:
            CONFIG["similarity_method"] = "Unknown"
        return func(*args, **kwargs)
    return wrapper

# =========================
# âš™ï¸ é˜ˆå€¼é…ç½®ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
# =========================
threshold_ratio = 0.85
#bm25
from concurrent.futures import ThreadPoolExecutor, as_completed

async def async_process_single_header(h_text: str, session: aiohttp.ClientSession) -> Dict:
    query = list(jieba.cut(h_text))
    scores = bm25.get_scores(query)
    max_global_score = max(scores)

    top_3_indices = np.argsort(scores)[-3:][::-1]
    top_3 = [standard_texts[i] for i in top_3_indices]
    top_scores = [scores[i] for i in top_3_indices]

    top_score = top_scores[0]

    if top_score < max_global_score * threshold_ratio:
        return {
            "åŸå§‹è¡¨å¤´": h_text,
            "å€™é€‰æœ¯è¯­": top_3,
            "LLMé€‰æ‹©": "",
            "æœ€é«˜ç›¸ä¼¼åº¦": round(top_score, 4),
            "æœ€é«˜åˆ†ç›¸å¯¹æ¯”ä¾‹ï¼ˆå½“å‰/maxï¼‰": round(top_score / max_global_score, 4) if max_global_score != 0 else 0,
            "æ˜¯å¦è°ƒç”¨LLM": "å¦"
        }
    else:
        prompt = f"""è¯·æ ¹æ®ç—…å†è¡¨å¤´é€‰æ‹©æœ€åŒ¹é…çš„æ ‡å‡†æœ¯è¯­ï¼š
åŸå§‹è¡¨å¤´ï¼š{h_text}
å€™é€‰æœ¯è¯­ï¼š
{chr(10).join(f'{i + 1}. {text}' for i, text in enumerate(top_3))}

åªéœ€è¿”å›é€‰æ‹©çš„ç¼–å·(1-3)ï¼Œä¸è¦è§£é‡Šã€‚"""
        llm_choice = await async_generate_with_llm(prompt, session)
        llm_choice_result = top_3[int(llm_choice) - 1] if llm_choice.isdigit() and 1 <= int(llm_choice) <= 3 else ""
        return {
            "åŸå§‹è¡¨å¤´": h_text,
            "å€™é€‰æœ¯è¯­": top_3,
            "LLMé€‰æ‹©": llm_choice_result,
            "æœ€é«˜ç›¸ä¼¼åº¦": round(top_score, 4),
            "æœ€é«˜åˆ†ç›¸å¯¹æ¯”ä¾‹ï¼ˆå½“å‰/maxï¼‰": round(top_score / max_global_score, 4) if max_global_score != 0 else 0,
            "æ˜¯å¦è°ƒç”¨LLM": "æ˜¯"
        }


@detect_similarity_method
def calculate_similarities_bm25() -> List[Dict]:
    global bm25, standard_texts
    header_texts = pd.read_csv(CONFIG["header_csv"], header=None)[0].dropna().astype(str).tolist()
    standard_texts = pd.read_csv(CONFIG["standard_terms_csv"])["å†…å®¹"].dropna().astype(str).tolist()
    tokenized_corpus = [list(jieba.cut(text)) for text in standard_texts]
    bm25 = BM25Okapi(tokenized_corpus)

    async def batch_process():
        results = [None] * len(header_texts)
        sem = asyncio.Semaphore(10)  # æ§åˆ¶æœ€å¤§å¹¶å‘é‡

        async with aiohttp.ClientSession() as session:
            async def limited_process(i, h_text):
                async with sem:
                    result = await async_process_single_header(h_text, session)
                    results[i] = result

            tasks = [limited_process(i, h_text) for i, h_text in enumerate(header_texts)]
            for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="âš¡ Async LLMä¸­"):
                await f

        return results

    return asyncio.run(batch_process())

    # å¤šçº¿ç¨‹æ‰§è¡Œï¼ˆçº¿ç¨‹æ± ï¼‰
    # å¤šçº¿ç¨‹æ‰§è¡Œï¼ˆçº¿ç¨‹æ± ï¼‰ï¼Œä¿è¯ç»“æœé¡ºåºä¸€è‡´


    def process_single_header_with_index(index, h_text):
        result = process_single_header(h_text)
        return index, result

    # æŒ‰åŸå§‹é¡ºåºåˆå§‹åŒ–ç©ºåˆ—è¡¨
    results = [None] * len(header_texts)

    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {
            executor.submit(process_single_header_with_index, idx, h_text): idx
            for idx, h_text in enumerate(header_texts)
        }

        with tqdm(total=len(header_texts), desc="ğŸ§  LLMåŒ¹é…ä¸­", ncols=80) as pbar:
            for future in as_completed(futures):
                try:
                    idx, result = future.result()
                    results[idx] = result
                except Exception as e:
                    print(f"âš ï¸ è¡¨å¤´å¤„ç†å¤±è´¥ï¼ˆindex={idx}ï¼‰: {e}")
                finally:
                    pbar.update(1)

    return results

def save_results(results: List[Dict]):
    df = pd.DataFrame(results)
    print("ä¿å­˜å‰çš„åˆ—åï¼š", df.columns.tolist())

    # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    df.drop(columns=[col for col in ["å¹³å‡ç›¸ä¼¼åº¦", "åŒ¹é…æˆåŠŸ"] if col in df.columns], inplace=True)

    # åŠ è½½ GT æ ‡å‡†ç­”æ¡ˆï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
    gt_path = "/home/gzy/rag-biomap/dataset/GT.xlsx"
    gt_df = pd.read_excel(gt_path, header=0)

    if gt_df.shape[1] < 2:
        raise ValueError("GT.xlsx å¿…é¡»è‡³å°‘åŒ…å«ä¸¤åˆ—ï¼Œç¬¬äºŒåˆ—ä¸ºæ ‡å‡†ç­”æ¡ˆ")

    gt_answers = gt_df.iloc[:, 1].fillna("").astype(str).tolist()
    df["GTæ ‡å‡†ç­”æ¡ˆ"] = pd.Series(gt_answers[:len(df)])

    # åŒ¹é…åˆ¤æ–­
    df["æ˜¯å¦åŒ¹é…GT"] = df.apply(lambda row: row["LLMé€‰æ‹©"].strip() == row["GTæ ‡å‡†ç­”æ¡ˆ"].strip(), axis=1)

    # ç»Ÿè®¡ä¿¡æ¯
    total_accuracy = df["æ˜¯å¦åŒ¹é…GT"].mean()
    gt_empty_count = sum(df["GTæ ‡å‡†ç­”æ¡ˆ"] == "")
    llm_empty = df["LLMé€‰æ‹©"] == ""
    gt_empty = df["GTæ ‡å‡†ç­”æ¡ˆ"] == ""
    llm_not_empty = df["LLMé€‰æ‹©"] != ""

    llm_empty_and_gt_empty = df[llm_empty & gt_empty].shape[0]
    llm_empty_total = llm_empty.sum()
    llm_not_empty_total = llm_not_empty.sum()
    llm_not_empty_gt_empty = df[llm_not_empty & gt_empty].shape[0]
    llm_empty_gt_not_empty = df[llm_empty & ~gt_empty].shape[0]

    # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯DataFrame
    stats_data = {
        "ç»Ÿè®¡æŒ‡æ ‡": [
            "llmé€‰æ‹©ä¸GTæ ‡å‡†ç­”æ¡ˆåŒ¹é…å‡†ç¡®ç‡",
            "GTæ ‡å‡†ç­”æ¡ˆä¸­ç©ºå€¼ä¸ªæ•°",
            "llmé€‰æ‹©ä¸ºç©ºï¼ŒGTä¹Ÿä¸ºç©ºçš„åŒ¹é…æˆåŠŸæ•°é‡",
            "llmé€‰æ‹©ä¸ºç©ºçš„æ•°é‡",
            "llmé€‰æ‹©éç©ºçš„æ•°é‡",
            "llmé€‰æ‹©éç©ºï¼Œä½†GTæ˜¯ç©ºçš„æ•°é‡",
            "llmé€‰æ‹©ä¸ºç©ºï¼ŒGTä¸ä¸ºç©ºçš„æ•°é‡"
        ],
        "æ•°å€¼": [
            total_accuracy,
            gt_empty_count,
            llm_empty_and_gt_empty,
            llm_empty_total,
            llm_not_empty_total,
            llm_not_empty_gt_empty,
            llm_empty_gt_not_empty
        ]
    }
    stats_df = pd.DataFrame(stats_data)

    # å°†ç»Ÿè®¡ä¿¡æ¯å†™å…¥Excelçš„ç¬¬12-15åˆ—ï¼ˆL-Oåˆ—ï¼‰
    with pd.ExcelWriter(os.path.join(CONFIG["output_dir"], "é˜ˆå€¼è®¾ç½®85%.xlsx"), engine="openpyxl") as writer:
        df.to_excel(writer, sheet_name="åŒ¹é…ç»“æœ", index=False)
        stats_df.to_excel(writer, sheet_name="åŒ¹é…ç»“æœ", startcol=11, startrow=1, index=False, header=False)

    # æ§åˆ¶å°è¾“å‡ºï¼ˆè¾…åŠ©ç¡®è®¤ï¼‰
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {os.path.join(CONFIG['output_dir'], 'é˜ˆå€¼è®¾ç½®85%.xlsx')}")
    print(f"ğŸ“Š åŒ¹é…å‡†ç¡®ç‡ï¼š{total_accuracy:.6f}")
    print(f"ğŸ“Š GTä¸ºç©ºå€¼ï¼š{gt_empty_count}ï¼Œllmé€‰æ‹©ä¸ºç©ºæ•°é‡ï¼š{llm_empty_total}")
    print(f"ğŸ“Š llmé€‰æ‹©ä¸ºç©º && GTä¸ºç©ºï¼ˆåŒ¹é…ï¼‰ï¼š{llm_empty_and_gt_empty}")
    print(f"ğŸ“Š llmé€‰æ‹©éç©º && GTä¸ºç©ºï¼š{llm_not_empty_gt_empty}")
    print(f"ğŸ“Š llmé€‰æ‹©ä¸ºç©º && GTéç©ºï¼š{llm_empty_gt_not_empty}")



def main():
    from Build_an_index.invoke_Build_index import get_embedding


    #è‡ªåŠ¨è¯†åˆ«å½“å‰ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹--æ–¹æ³•ï¼šè·å–get_embeddingå‡½æ•°çš„æ¥æºæ¨¡å—è·¯å¾„ï¼ˆæ¯”å¦‚ bge_m3.pyï¼‰
    module_path = get_embedding.__module__

    print(f"ğŸ“ æ£€æµ‹åˆ° get_embedding æ¥è‡ªæ¨¡å—ï¼š{module_path}")  # âœ… ï¼šè°ƒè¯•è¾“å‡º

    if "bge_m3" in module_path:
        CONFIG["embedding_model"] = "bge-m3"
    elif "nomic_embed_text" in module_path:
        CONFIG["embedding_model"] = "nomic-embed-text"
    elif "mxbai_embed_large" in module_path:
        CONFIG["embedding_model"] = "mxbai-embed-large"
    else:
        CONFIG["embedding_model"] = "unknown"

    #CONFIG["embedding_model"] = get_embedding.__defaults__[0]

    # è‡ªåŠ¨æ£€æµ‹å½“å‰å¯ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°æ˜¯å“ªä¸€ä¸ª-é€šè¿‡å…¨å±€å˜é‡è¡¨globals()æ¥æ£€æµ‹å“ªä¸ªå‡½æ•°å­˜åœ¨ï¼Œæ˜¯æˆ‘æœ‰æ²¡æœ‰â€œå®šä¹‰â€å®ƒã€‚
    if "calculate_similarities_bm25" in globals():
        calculate_similarities = calculate_similarities_bm25
    elif "calculate_similarities_cosine" in globals():
        calculate_similarities = calculate_similarities_cosine
    else:
        raise ValueError(
            "æœªæ£€æµ‹åˆ°ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼Œè¯·ç¡®ä¿ä¿ç•™ calculate_similarities_bm25 æˆ– calculate_similarities_cosine ä¸­çš„ä¸€ä¸ª")

    initialize_directories()
    print("ğŸ”„ å¤„ç†éæ ‡å‡†æ•°æ®...")
    header_texts = process_non_standard_data()

    print("ğŸ”„ å¤„ç†æ ‡å‡†çŸ¥è¯†åº“...")
    standard_texts = process_standard_data()

    print("ğŸ” è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ‰§è¡Œç®€åŒ–RAGæµç¨‹...")
    results = calculate_similarities()

#è¿è¡Œç»“æœæ˜¾ç¤ºï¼šğŸ“ å½“å‰ä½¿ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ä¸ºï¼š......
    print(f"ğŸ“ å½“å‰ä½¿ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ä¸ºï¼š{CONFIG['similarity_method']}")

    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_results(results)

if __name__ == "__main__":
    main()
